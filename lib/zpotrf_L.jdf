extern "C" %{
/*
 * Copyright (c) 2010-2013 The University of Tennessee and The University
 *                         of Tennessee Research Foundation.  All rights
 *                         reserved.
 * Copyright (c) 2013      Inria. All rights reserved.
 *
 * @precisions normal z -> s d c
 *
 */
#include "dplasma/lib/dplasmajdf.h"
#include "data_dist/matrix/matrix.h"

#if defined(HAVE_RECURSIVE)
#include "data_dist/matrix/subtile.h"
#include "dague/recursive.h"
#endif

#if defined(HAVE_CUDA)
#include "dague/devices/cuda/dev_cuda.h"
#include "dplasma/cores/cuda_zgemm.h"
#endif  /* defined(HAVE_CUDA) */

/*
 * Priorities used in this jdf:
 *      - potrf_zpotrf(k)    : (MT-k)**3
 *      - potrf_zherk(k,m)   : (MT-m)**3 + 3 * (m - k)
 *      - potrf_ztrsm(m,k)   : (MT-m)**3 + 3 * (m - k) * (2 * MT - k - m - 1)
 *      - potrf_zgemm(m,n,k) : (MT-m)**3 + 3 * (m - n) * (2 * MT - m - n - 1) + 6 * (m - k)
 *
 * So max priority is:
 *      (MT - PRI_CHANGE)**3 + 3 * MT * (2 * MT - PRI_CHANGE - 1) + 6 * MT  < (MT**3 + 6 MT**2 + 3 MT)
 *
 * WARNING: If mt is greater than 1200, we might get integer overflow.
 */

/**
 * Example of user-defined hash function.
 * NB: the function is used in the generated code, so the user needs to provide the prototype;
 *     however necessary datatypes (internal handle and function-specific assignment type)
 *     will only be generated after the prologue is dumped, so the user needs to provide
 *     the prototype in the prologue, and the implementation in the epilogue.
 */
struct __dague_zpotrf_L_internal_handle;
struct __dague_zpotrf_L_potrf_zpotrf_assignment_s;
static uint64_t my_potrf_hash(const struct __dague_zpotrf_L_internal_handle * __dague_handle,
                              struct __dague_zpotrf_L_potrf_zpotrf_assignment_s * assignments);
static int my_potrf_startup(dague_context_t * context,
                            struct __dague_zpotrf_L_internal_handle *__dague_handle,
                            dague_execution_context_t ** pready_list);
%}

/* Globals
 */
uplo       [type = PLASMA_enum]
dataA      [type = "dague_ddesc_t *"]
INFO       [type = "int*"]

descA      [type = "tiled_matrix_desc_t" hidden = on default = "*((tiled_matrix_desc_t*)dataA)"]
PRI_CHANGE [type = "int" hidden = on default = 0 ]
PRI_MAX    [type = "int" hidden = on default = "(descA.mt * ( 3 + descA.mt * ( 2 + descA.mt )))" ]
smallnb    [type = "int" hidden = on default = "descA.mb" ]

/**************************************************
 *               potrf_zpotrf                     *
 **************************************************/
potrf_zpotrf(k) [high_priority = on hash_fn = my_potrf_hash startup_fn = my_potrf_startup]

// Execution space
k = 0 .. descA.mt-1

// Parallel partitioning
:dataA(k, k)

// Parameters
RW T <- (k == 0) ? dataA(k, k) : T potrf_zherk(k-1, k)
     -> T potrf_ztrsm(k+1..descA.mt-1, k)
     -> dataA(k, k)

; (k >= (descA.mt - PRI_CHANGE)) ? (descA.mt - k) * (descA.mt - k) * (descA.mt - k) : PRI_MAX

BODY [type=RECURSIVE]
{
    int tempkm = k == descA.mt-1 ? descA.m - k*descA.mb : descA.mb;
    int iinfo = 0;

    if (tempkm > smallnb)
    {
        subtile_desc_t *small_descT;
        dague_handle_t *dague_zpotrf;

        small_descT = subtile_desc_create( &(descA), k, k,
                                           smallnb, smallnb, 0, 0, tempkm, tempkm );
        small_descT->mat = T;

        dague_zpotrf = dplasma_zpotrf_New(uplo, (tiled_matrix_desc_t *)small_descT, &iinfo );

        dague_recursivecall( context, this_task,
                             dague_zpotrf, dplasma_zpotrf_Destruct,
                             1, small_descT );

        return DAGUE_HOOK_RETURN_AGAIN;
    }
    else
        /* Go for the sequential CPU version */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY
{
    int tempkm = k == descA.mt-1 ? descA.m - k*descA.mb : descA.mb;
    int iinfo = 0;
    int ldak = BLKLDD( descA, k );

#if !defined(DAGUE_DRY_RUN)
    CORE_zpotrf( uplo, tempkm, T, ldak, &iinfo );
    if ( iinfo != 0 && *INFO == 0 )
            *INFO = k*descA.mb+iinfo; /* Should return here */
#endif /* !defined(DAGUE_DRY_RUN) */

    printlog("CORE_zpotrf( %d )\n\t( %s, %d, A(%d,%d)[%p], %d) return info = %d\n",
             k,
             plasma_const(uplo), tempkm, k, k, T, descA.mb, iinfo );
}
END


/**************************************************
 *               potrf_ztrsm                      *
 **************************************************/
potrf_ztrsm(m, k) [high_priority = on]

// Execution space
m = 1 .. descA.mt-1
k = 0 .. m-1

// Parallel partitioning
: dataA(m, k)

// Parameters
READ  T <- T potrf_zpotrf(k)
RW    C <- (k == 0) ? dataA(m, k) : C potrf_zgemm(m, k, k-1)
        -> A potrf_zherk(k, m)
        -> A potrf_zgemm(m, k+1..m-1, k)
        -> B potrf_zgemm(m+1..descA.mt-1, m, k)
        -> dataA(m, k)

; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - k - m - 1) * (m - k) : PRI_MAX

BODY [type=RECURSIVE]
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;

    if ( (tempmm > smallnb) || (descA.nb > smallnb) )
    {
        subtile_desc_t *small_descT;
        subtile_desc_t *small_descC;
        dague_handle_t* dague_ztrsm;

        dague_data_transfer_ownership_to_copy(gC->original, 0 /* device */, FLOW_ACCESS_READ | FLOW_ACCESS_WRITE);

        small_descT = subtile_desc_create( &(descA), k, k,
                                           smallnb, smallnb, 0, 0, descA.nb, descA.nb );
        small_descT->mat = T;

        small_descC = subtile_desc_create( &(descA), m, k,
                                           smallnb, smallnb, 0, 0, tempmm, descA.nb );
        small_descC->mat = C;

        dague_ztrsm = dplasma_ztrsm_New(PlasmaRight, PlasmaLower,
                                        PlasmaConjTrans, PlasmaNonUnit,
                                        (dague_complex64_t)1.0,
                                        (tiled_matrix_desc_t *)small_descT,
                                        (tiled_matrix_desc_t *)small_descC );

        dague_recursivecall( context, this_task,
                             dague_ztrsm, dplasma_ztrsm_Destruct,
                             2, small_descT, small_descC );

        return DAGUE_HOOK_RETURN_AGAIN;
    }
    else
        /* Go for the sequential CPU version */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    int ldak = BLKLDD( descA, k );
    int ldam = BLKLDD( descA, m );

    dague_data_transfer_ownership_to_copy(gC->original, 0 /* device */, FLOW_ACCESS_READ | FLOW_ACCESS_WRITE);

#if !defined(DAGUE_DRY_RUN)
    CORE_ztrsm(PlasmaRight, PlasmaLower, PlasmaConjTrans, PlasmaNonUnit,
               tempmm, descA.nb,
               (dague_complex64_t)1.0, T /*A(k, k)*/, ldak,
                                       C /*A(m, k)*/, ldam);
#endif  /* !defined(DAGUE_DRY_RUN) */

    printlog("CORE_ztrsm( %d, %d )\n\t( %s, %s, %s, %s, %d, %d, %f, A(%d,%d)[%p], %d,  A(%d,%d)[%p], %d)\n",
             m, k,
             plasma_const( PlasmaRight ), plasma_const( PlasmaLower ),
             plasma_const( PlasmaConjTrans ), plasma_const( PlasmaNonUnit ),
             tempmm, descA.nb,
             1.0, k, k, T, ldak,
                  m, k, C, ldam);
}
END


/**************************************************
 *               potrf_zherk                      *
 **************************************************/
potrf_zherk(k, m) [high_priority = on]

// Execution space
k = 0   .. descA.mt-2
m = k+1 .. descA.mt-1

// Parallel partitioning
: dataA(m, m)

//Parameters
READ  A <- C potrf_ztrsm(m, k)
RW    T <- (k == 0)   ? dataA(m, m) : T potrf_zherk(k-1, m)
        -> (m == k+1) ? T potrf_zpotrf(m)  : T potrf_zherk(k+1, m)

; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * (m - k) : PRI_MAX

BODY [type=RECURSIVE]
{
    int tempmm = m == descA.mt-1 ? descA.m - m*descA.mb : descA.mb;

    if ( (tempmm > smallnb) || (descA.nb > smallnb) )
    {
        subtile_desc_t *small_descT;
        subtile_desc_t *small_descA;
        dague_handle_t* dague_zherk;

        small_descT = subtile_desc_create( &(descA), m, m,
                                           smallnb, smallnb, 0, 0, tempmm, tempmm );
        small_descT->mat = T;

        small_descA = subtile_desc_create( &(descA), m, k,
                                           smallnb, smallnb, 0, 0, tempmm, descA.nb );
        small_descA->mat = A;

        dague_zherk = dplasma_zherk_New( PlasmaLower, PlasmaNoTrans,
                                         (double)-1.0, (tiled_matrix_desc_t*) small_descA,
                                         (double)1.0,  (tiled_matrix_desc_t*) small_descT);

        dague_recursivecall( context, this_task,
                             dague_zherk, dplasma_zherk_Destruct,
                             2, small_descA, small_descT );
        return DAGUE_HOOK_RETURN_AGAIN;
    }
    else
        /* Go for the sequential CPU version */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY
{
    int tempmm = m == descA.mt-1 ? descA.m - m*descA.mb : descA.mb;
    int ldam = BLKLDD( descA, m );

#if !defined(DAGUE_DRY_RUN)
    CORE_zherk(PlasmaLower, PlasmaNoTrans,
               tempmm, descA.mb,
               (double)-1.0, A /*A(m, k)*/, ldam,
               (double) 1.0, T /*A(m, m)*/, ldam);
#endif  /* !defined(DAGUE_DRY_RUN) */
    printlog(
             "CORE_zherk( %d, %d )\n\t( %s, %s, %d, %d, %f, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             k, m,
             plasma_const( PlasmaLower ), plasma_const( PlasmaNoTrans ),
             tempmm, descA.mb,
             -1.0, m, k, A, ldam,
              1.0, m, m, T, ldam);
}
END

/**************************************************
 *               potrf_zgemm                      *
 **************************************************/
// Name
potrf_zgemm(m, n, k)

// Execution space
k = 0   .. descA.mt-3
m = k+2 .. descA.mt-1
n = k+1 .. m-1

// Parallel partitioning
: dataA(m, n)

// Parameters
READ  A <- C potrf_ztrsm(m, k)
READ  B <- C potrf_ztrsm(n, k)
RW    C <- (k == 0)   ? dataA(m, n)  : C potrf_zgemm(m, n, k-1)
        -> (n == k+1) ? C potrf_ztrsm(m, n) : C potrf_zgemm(m, n, k+1)

; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - m - n - 3) * (m - n) + 6 * (m - k) : PRI_MAX

BODY [type=CUDA dyld=cublasZgemm]
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    int ldam = BLKLDD( descA, m );
    int ldan = BLKLDD( descA, n );
    int ret;

    /*
     * Transfer to GPU, only if ld == mb, and we are in tile storage, otherwise
     * it is a recursive call and we have lapack layout storage with partial data
     */
    if ( !(ldam > descA.mb) )
    {
        ret = gpu_zgemm(context, (dague_execution_context_t*)this_task,
                        ( n == k+1 ), n-k,
                        PlasmaNoTrans, PlasmaConjTrans,
                        tempmm, descA.mb, descA.mb,
                        (dague_complex64_t)-1.0, ldam,
                                                 ldan,
                        (dague_complex64_t) 1.0, ldam);

        printlog("CUDA_zgemm( %d, %d, %d )\n\t( %s, %s, %d, %d, %d, %f, A(%d,%d)[%p], %d, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             m, n, k,
             plasma_const( PlasmaNoTrans ),  plasma_const( PlasmaConjTrans ),
             tempmm, descA.mb, descA.mb,
             -1.0, m, k, A, ldam,
                   n, k, B, ldan,
              1.0, m, n, C, ldam);

        return ret;
    }
    else
        /* Go to next kernel */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY [type=RECURSIVE]
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;

    if ( (tempmm > smallnb) || (descA.nb > smallnb) )
    {
        subtile_desc_t *small_descA;
        subtile_desc_t *small_descB;
        subtile_desc_t *small_descC;
        dague_handle_t *dague_zgemm;

        small_descA = subtile_desc_create( &(descA), m, k,
                                           smallnb, smallnb, 0, 0, tempmm, descA.nb );
        small_descA->mat = A;

        small_descB = subtile_desc_create( &(descA), n, k,
                                           smallnb, smallnb, 0, 0, descA.mb, descA.nb );
        small_descB->mat = B;

        small_descC = subtile_desc_create( &(descA), m, n,
                                           smallnb, smallnb, 0, 0, tempmm, descA.nb );
        small_descC->mat = C;

        dague_zgemm = dplasma_zgemm_New(PlasmaNoTrans, PlasmaConjTrans,
                                        (dague_complex64_t)-1.0,
                                        (tiled_matrix_desc_t *)small_descA,
                                        (tiled_matrix_desc_t *)small_descB,
                                        (dague_complex64_t) 1.0,
                                        (tiled_matrix_desc_t *)small_descC);

        dague_recursivecall( context, this_task,
                             dague_zgemm, dplasma_zgemm_Destruct,
                             3, small_descA, small_descB, small_descC );

        return DAGUE_HOOK_RETURN_AGAIN;
    }
    else
        /* Go to CPU sequential kernel */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    int ldam = BLKLDD( descA, m );
    int ldan = BLKLDD( descA, n );

#if !defined(DAGUE_DRY_RUN)
    CORE_zgemm(PlasmaNoTrans, PlasmaConjTrans,
               tempmm, descA.mb, descA.mb,
               (dague_complex64_t)-1.0, A /*A(m, k)*/, ldam,
                                        B /*A(n, k)*/, ldan,
               (dague_complex64_t) 1.0, C /*A(m, n)*/, ldam);
#endif  /* !defined(DAGUE_DRY_RUN) */

    printlog("CORE_zgemm( %d, %d, %d )\n\t( %s, %s, %d, %d, %d, %f, A(%d,%d)[%p], %d, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             m, n, k,
             plasma_const( PlasmaNoTrans ),  plasma_const( PlasmaConjTrans ),
             tempmm, descA.mb, descA.mb,
             -1.0, m, k, A, ldam,
                   n, k, B, ldan,
              1.0, m, n, C, ldam);
}
END

extern "C" %{
static uint64_t my_potrf_hash(const struct __dague_zpotrf_L_internal_handle * __dague_handle,
                              struct __dague_zpotrf_L_potrf_zpotrf_assignment_s * assignments)
{
   (void)__dague_handle;
   return (uint64_t)(assignments->k.value);
}

static int my_potrf_startup(dague_context_t * context,
                            __dague_zpotrf_L_internal_handle_t *__dague_handle,
                            dague_execution_context_t ** pready_list)
{
  __dague_zpotrf_L_potrf_zpotrf_task_t *new_context;
  int vpid;

  if (!potrf_zpotrf_pred(0))
	    return 0;

  new_context = (__dague_zpotrf_L_potrf_zpotrf_task_t *) dague_lifo_pop(
      &context->virtual_processes[0]->execution_units[0]->context_mempool->mempool);
  if (NULL == new_context)
      new_context = (__dague_zpotrf_L_potrf_zpotrf_task_t *) dague_thread_mempool_allocate(
            context->virtual_processes[0]->execution_units[0]->context_mempool);
  new_context->dague_handle = (dague_handle_t *) __dague_handle;
  new_context->function = __dague_handle->super.super.functions_array[zpotrf_L_potrf_zpotrf.function_id];
  new_context->chore_id = 0;
  new_context->locals.k.value = 0;
  DAGUE_LIST_ITEM_SINGLETON(new_context);
  new_context->priority = 0;
  new_context->data.T.data_repo = NULL;
  new_context->data.T.data_in = NULL;
  new_context->data.T.data_out = NULL;
  dague_dependencies_mark_task_as_startup((dague_execution_context_t *) new_context);
  vpid = ((dague_ddesc_t *) __dague_handle->super.dataA)->vpid_of(
                      (dague_ddesc_t *) __dague_handle->super.dataA, 0, 0);
  if (NULL != pready_list[vpid]) {
      dague_list_item_ring_merge((dague_list_item_t *) new_context,
                                 (dague_list_item_t *) (pready_list[vpid]));
  }
  pready_list[vpid] = (dague_execution_context_t *) new_context;
  return 0;
}
%}
