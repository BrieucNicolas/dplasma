extern "C" %{
/*
 * Copyright (c) 2010-2013 The University of Tennessee and The University
 *                         of Tennessee Research Foundation.  All rights
 *                         reserved.
 * Copyright (c) 2013      Inria. All rights reserved.
 *
 * @precisions normal z -> s d c
 *
 */
#include "dplasma/lib/dplasmajdf.h"
#include "data_dist/matrix/matrix.h"

#if defined(HAVE_RECURSIVE)
#include "data_dist/matrix/subtile.h"
#include "dague/recursive.h"
#endif

#if defined(HAVE_CUDA)
#include "dague/devices/cuda/dev_cuda.h"
#include "dplasma/cores/cuda_zgemm.h"
#endif  /* defined(HAVE_CUDA) */

/*
 * Priorities used in this jdf:
 *      - potrf_zpotrf(k)    : (MT-k)**3
 *      - potrf_zherk(k,m)   : (MT-m)**3 + 3 * (m - k)
 *      - potrf_ztrsm(m,k)   : (MT-m)**3 + 3 * (m - k) * (2 * MT - k - m - 1)
 *      - potrf_zgemm(m,n,k) : (MT-m)**3 + 3 * (m - n) * (2 * MT - m - n - 1) + 6 * (m - k)
 *
 * So max priority is:
 *      (MT - PRI_CHANGE)**3 + 3 * MT * (2 * MT - PRI_CHANGE - 1) + 6 * MT  < (MT**3 + 6 MT**2 + 3 MT)
 *
 * WARNING: If mt is greater than 1200, we might get integer overflow.
 */

/**
 * Example of user-defined hash function.
 * NB: the function is used in the generated code, so the user needs to provide the prototype;
 *     however necessary datatypes (internal handle and function-specific assignment type)
 *     will only be generated after the prologue is dumped, so the user needs to provide
 *     the prototype in the prologue, and the implementation in the epilogue.
 */
struct __dague_zpotrf_L_internal_handle;
struct __dague_zpotrf_L_potrf_zpotrf_assignment_s;
static uint64_t my_potrf_hash(const struct __dague_zpotrf_L_internal_handle * __dague_handle,
                              struct __dague_zpotrf_L_potrf_zpotrf_assignment_s * assignments);
static int my_potrf_startup(dague_context_t * context,
                            struct __dague_zpotrf_L_internal_handle *__dague_handle,
                            dague_execution_context_t ** pready_list);
static uint32_t my_nb_local_tasks_gemm(struct __dague_zpotrf_L_internal_handle *__dague_handle);
static uint32_t my_nb_local_tasks_trsm(struct __dague_zpotrf_L_internal_handle *__dague_handle);
static uint32_t my_nb_local_tasks_potrf(struct __dague_zpotrf_L_internal_handle *__dague_handle);
static uint32_t my_nb_local_tasks_herk(struct __dague_zpotrf_L_internal_handle *__dague_handle);

static void               *my_potrf_deps_allocate(struct __dague_zpotrf_L_internal_handle *__dague_handle);
static void                my_potrf_deps_free(struct __dague_zpotrf_L_internal_handle *__dague_handle, void *deps);
static dague_dependency_t *my_potrf_deps_find(const dague_handle_t *dague_handle,
                                              const dague_execution_context_t* restrict exec_context);
%}

/* Globals
 */
uplo       [type = PLASMA_enum]
dataA      [type = "dague_ddesc_t *"]
INFO       [type = "int*"]

descA      [type = "tiled_matrix_desc_t" hidden = on default = "*((tiled_matrix_desc_t*)dataA)"]
PRI_CHANGE [type = "int" hidden = on default = 0 ]
PRI_MAX    [type = "int" hidden = on default = "(descA.mt * ( 3 + descA.mt * ( 2 + descA.mt )))" ]
smallnb    [type = "int" hidden = on default = "descA.mb" ]

/**************************************************
 *               potrf_zpotrf                     *
 **************************************************/
potrf_zpotrf(k) [high_priority = on
                 hash_fn = my_potrf_hash
                 startup_fn = my_potrf_startup
                 nb_local_tasks_fn = my_nb_local_tasks_potrf
                 find_deps_fn  = my_potrf_deps_find
                 alloc_deps_fn = my_potrf_deps_allocate
                 free_deps_fn  = my_potrf_deps_free ]

// Execution space
k = 0 .. descA.mt-1

// Parallel partitioning
:dataA(k, k)

// Parameters
RW T <- (k == 0) ? dataA(k, k) : T potrf_zherk(k-1, k)
     -> T potrf_ztrsm(k+1..descA.mt-1, k)
     -> dataA(k, k)

; (k >= (descA.mt - PRI_CHANGE)) ? (descA.mt - k) * (descA.mt - k) * (descA.mt - k) : PRI_MAX

BODY [type=RECURSIVE]
{
    int tempkm = k == descA.mt-1 ? descA.m - k*descA.mb : descA.mb;
    int iinfo = 0;

    if (tempkm > smallnb)
    {
        subtile_desc_t *small_descT;
        dague_handle_t *dague_zpotrf;

        small_descT = subtile_desc_create( &(descA), k, k,
                                           smallnb, smallnb, 0, 0, tempkm, tempkm );
        small_descT->mat = T;

        dague_zpotrf = dplasma_zpotrf_New(uplo, (tiled_matrix_desc_t *)small_descT, &iinfo );

        dague_recursivecall( context, this_task,
                             dague_zpotrf, dplasma_zpotrf_Destruct,
                             1, small_descT );

        return DAGUE_HOOK_RETURN_AGAIN;
    }
    else
        /* Go for the sequential CPU version */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY
{
    int tempkm = k == descA.mt-1 ? descA.m - k*descA.mb : descA.mb;
    int iinfo = 0;
    int ldak = BLKLDD( descA, k );

#if !defined(DAGUE_DRY_RUN)
    CORE_zpotrf( uplo, tempkm, T, ldak, &iinfo );
    if ( iinfo != 0 && *INFO == 0 )
            *INFO = k*descA.mb+iinfo; /* Should return here */
#endif /* !defined(DAGUE_DRY_RUN) */

    printlog("CORE_zpotrf( %d )\n\t( %s, %d, A(%d,%d)[%p], %d) return info = %d\n",
             k,
             plasma_const(uplo), tempkm, k, k, T, descA.mb, iinfo );
}
END


/**************************************************
 *               potrf_ztrsm                      *
 **************************************************/
potrf_ztrsm(m, k) [high_priority = on nb_local_tasks_fn = my_nb_local_tasks_trsm]

// Execution space
m = 1 .. descA.mt-1
k = 0 .. m-1

// Parallel partitioning
: dataA(m, k)

// Parameters
READ  T <- T potrf_zpotrf(k)
RW    C <- (k == 0) ? dataA(m, k) : C potrf_zgemm(m, k, k-1)
        -> A potrf_zherk(k, m)
        -> A potrf_zgemm(m, k+1..m-1, k)
        -> B potrf_zgemm(m+1..descA.mt-1, m, k)
        -> dataA(m, k)

; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - k - m - 1) * (m - k) : PRI_MAX

BODY [type=RECURSIVE]
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;

    if ( (tempmm > smallnb) || (descA.nb > smallnb) )
    {
        subtile_desc_t *small_descT;
        subtile_desc_t *small_descC;
        dague_handle_t* dague_ztrsm;

        dague_data_transfer_ownership_to_copy(gC->original, 0 /* device */, FLOW_ACCESS_READ | FLOW_ACCESS_WRITE);

        small_descT = subtile_desc_create( &(descA), k, k,
                                           smallnb, smallnb, 0, 0, descA.nb, descA.nb );
        small_descT->mat = T;

        small_descC = subtile_desc_create( &(descA), m, k,
                                           smallnb, smallnb, 0, 0, tempmm, descA.nb );
        small_descC->mat = C;

        dague_ztrsm = dplasma_ztrsm_New(PlasmaRight, PlasmaLower,
                                        PlasmaConjTrans, PlasmaNonUnit,
                                        (dague_complex64_t)1.0,
                                        (tiled_matrix_desc_t *)small_descT,
                                        (tiled_matrix_desc_t *)small_descC );

        dague_recursivecall( context, this_task,
                             dague_ztrsm, dplasma_ztrsm_Destruct,
                             2, small_descT, small_descC );

        return DAGUE_HOOK_RETURN_AGAIN;
    }
    else
        /* Go for the sequential CPU version */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    int ldak = BLKLDD( descA, k );
    int ldam = BLKLDD( descA, m );

    dague_data_transfer_ownership_to_copy(gC->original, 0 /* device */, FLOW_ACCESS_READ | FLOW_ACCESS_WRITE);

#if !defined(DAGUE_DRY_RUN)
    CORE_ztrsm(PlasmaRight, PlasmaLower, PlasmaConjTrans, PlasmaNonUnit,
               tempmm, descA.nb,
               (dague_complex64_t)1.0, T /*A(k, k)*/, ldak,
                                       C /*A(m, k)*/, ldam);
#endif  /* !defined(DAGUE_DRY_RUN) */

    printlog("CORE_ztrsm( %d, %d )\n\t( %s, %s, %s, %s, %d, %d, %f, A(%d,%d)[%p], %d,  A(%d,%d)[%p], %d)\n",
             m, k,
             plasma_const( PlasmaRight ), plasma_const( PlasmaLower ),
             plasma_const( PlasmaConjTrans ), plasma_const( PlasmaNonUnit ),
             tempmm, descA.nb,
             1.0, k, k, T, ldak,
                  m, k, C, ldam);
}
END


/**************************************************
 *               potrf_zherk                      *
 **************************************************/
potrf_zherk(k, m) [high_priority = on nb_local_tasks_fn = my_nb_local_tasks_herk]

// Execution space
k = 0   .. descA.mt-2
m = k+1 .. descA.mt-1

// Parallel partitioning
: dataA(m, m)

//Parameters
READ  A <- C potrf_ztrsm(m, k)
RW    T <- (k == 0)   ? dataA(m, m) : T potrf_zherk(k-1, m)
        -> (m == k+1) ? T potrf_zpotrf(m)  : T potrf_zherk(k+1, m)

; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * (m - k) : PRI_MAX

BODY [type=RECURSIVE]
{
    int tempmm = m == descA.mt-1 ? descA.m - m*descA.mb : descA.mb;

    if ( (tempmm > smallnb) || (descA.nb > smallnb) )
    {
        subtile_desc_t *small_descT;
        subtile_desc_t *small_descA;
        dague_handle_t* dague_zherk;

        small_descT = subtile_desc_create( &(descA), m, m,
                                           smallnb, smallnb, 0, 0, tempmm, tempmm );
        small_descT->mat = T;

        small_descA = subtile_desc_create( &(descA), m, k,
                                           smallnb, smallnb, 0, 0, tempmm, descA.nb );
        small_descA->mat = A;

        dague_zherk = dplasma_zherk_New( PlasmaLower, PlasmaNoTrans,
                                         (double)-1.0, (tiled_matrix_desc_t*) small_descA,
                                         (double)1.0,  (tiled_matrix_desc_t*) small_descT);

        dague_recursivecall( context, this_task,
                             dague_zherk, dplasma_zherk_Destruct,
                             2, small_descA, small_descT );
        return DAGUE_HOOK_RETURN_AGAIN;
    }
    else
        /* Go for the sequential CPU version */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY
{
    int tempmm = m == descA.mt-1 ? descA.m - m*descA.mb : descA.mb;
    int ldam = BLKLDD( descA, m );

#if !defined(DAGUE_DRY_RUN)
    CORE_zherk(PlasmaLower, PlasmaNoTrans,
               tempmm, descA.mb,
               (double)-1.0, A /*A(m, k)*/, ldam,
               (double) 1.0, T /*A(m, m)*/, ldam);
#endif  /* !defined(DAGUE_DRY_RUN) */
    printlog(
             "CORE_zherk( %d, %d )\n\t( %s, %s, %d, %d, %f, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             k, m,
             plasma_const( PlasmaLower ), plasma_const( PlasmaNoTrans ),
             tempmm, descA.mb,
             -1.0, m, k, A, ldam,
              1.0, m, m, T, ldam);
}
END

/**************************************************
 *               potrf_zgemm                      *
 **************************************************/
// Name
potrf_zgemm(m, n, k) [ nb_local_tasks_fn = my_nb_local_tasks_gemm ]

// Execution space
k = 0   .. descA.mt-3
m = k+2 .. descA.mt-1
n = k+1 .. m-1

// Parallel partitioning
: dataA(m, n)

// Parameters
READ  A <- C potrf_ztrsm(m, k)
READ  B <- C potrf_ztrsm(n, k)
RW    C <- (k == 0)   ? dataA(m, n)  : C potrf_zgemm(m, n, k-1)
        -> (n == k+1) ? C potrf_ztrsm(m, n) : C potrf_zgemm(m, n, k+1)

; (m >= (descA.mt - PRI_CHANGE)) ? (descA.mt - m) * (descA.mt - m) * (descA.mt - m) + 3 * ((2 * descA.mt) - m - n - 3) * (m - n) + 6 * (m - k) : PRI_MAX

BODY [type=CUDA dyld=cublasZgemm]
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    int ldam = BLKLDD( descA, m );
    int ldan = BLKLDD( descA, n );
    int ret;

    /*
     * Transfer to GPU, only if ld == mb, and we are in tile storage, otherwise
     * it is a recursive call and we have lapack layout storage with partial data
     */
    if ( !(ldam > descA.mb) )
    {
        ret = gpu_zgemm(context, (dague_execution_context_t*)this_task,
                        ( n == k+1 ), n-k,
                        PlasmaNoTrans, PlasmaConjTrans,
                        tempmm, descA.mb, descA.mb,
                        (dague_complex64_t)-1.0, ldam,
                                                 ldan,
                        (dague_complex64_t) 1.0, ldam);

        printlog("CUDA_zgemm( %d, %d, %d )\n\t( %s, %s, %d, %d, %d, %f, A(%d,%d)[%p], %d, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             m, n, k,
             plasma_const( PlasmaNoTrans ),  plasma_const( PlasmaConjTrans ),
             tempmm, descA.mb, descA.mb,
             -1.0, m, k, A, ldam,
                   n, k, B, ldan,
              1.0, m, n, C, ldam);

        return ret;
    }
    else
        /* Go to next kernel */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY [type=RECURSIVE]
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;

    if ( (tempmm > smallnb) || (descA.nb > smallnb) )
    {
        subtile_desc_t *small_descA;
        subtile_desc_t *small_descB;
        subtile_desc_t *small_descC;
        dague_handle_t *dague_zgemm;

        small_descA = subtile_desc_create( &(descA), m, k,
                                           smallnb, smallnb, 0, 0, tempmm, descA.nb );
        small_descA->mat = A;

        small_descB = subtile_desc_create( &(descA), n, k,
                                           smallnb, smallnb, 0, 0, descA.mb, descA.nb );
        small_descB->mat = B;

        small_descC = subtile_desc_create( &(descA), m, n,
                                           smallnb, smallnb, 0, 0, tempmm, descA.nb );
        small_descC->mat = C;

        dague_zgemm = dplasma_zgemm_New(PlasmaNoTrans, PlasmaConjTrans,
                                        (dague_complex64_t)-1.0,
                                        (tiled_matrix_desc_t *)small_descA,
                                        (tiled_matrix_desc_t *)small_descB,
                                        (dague_complex64_t) 1.0,
                                        (tiled_matrix_desc_t *)small_descC);

        dague_recursivecall( context, this_task,
                             dague_zgemm, dplasma_zgemm_Destruct,
                             3, small_descA, small_descB, small_descC );

        return DAGUE_HOOK_RETURN_AGAIN;
    }
    else
        /* Go to CPU sequential kernel */
        return DAGUE_HOOK_RETURN_NEXT;
}
END

BODY
{
    int tempmm = m == descA.mt-1 ? descA.m - m * descA.mb : descA.mb;
    int ldam = BLKLDD( descA, m );
    int ldan = BLKLDD( descA, n );

#if !defined(DAGUE_DRY_RUN)
    CORE_zgemm(PlasmaNoTrans, PlasmaConjTrans,
               tempmm, descA.mb, descA.mb,
               (dague_complex64_t)-1.0, A /*A(m, k)*/, ldam,
                                        B /*A(n, k)*/, ldan,
               (dague_complex64_t) 1.0, C /*A(m, n)*/, ldam);
#endif  /* !defined(DAGUE_DRY_RUN) */

    printlog("CORE_zgemm( %d, %d, %d )\n\t( %s, %s, %d, %d, %d, %f, A(%d,%d)[%p], %d, A(%d,%d)[%p], %d, %f, A(%d,%d)[%p], %d)\n",
             m, n, k,
             plasma_const( PlasmaNoTrans ),  plasma_const( PlasmaConjTrans ),
             tempmm, descA.mb, descA.mb,
             -1.0, m, k, A, ldam,
                   n, k, B, ldan,
              1.0, m, n, C, ldam);
}
END

extern "C" %{
static uint64_t my_potrf_hash(const struct __dague_zpotrf_L_internal_handle * __dague_handle,
                              struct __dague_zpotrf_L_potrf_zpotrf_assignment_s * assignments)
{
   (void)__dague_handle;
   return (uint64_t)(assignments->k.value);
}

static int my_potrf_startup(dague_context_t * context,
                            __dague_zpotrf_L_internal_handle_t *__dague_handle,
                            dague_execution_context_t ** pready_list)
{
  __dague_zpotrf_L_potrf_zpotrf_task_t *new_context;
  int vpid;

  if (!potrf_zpotrf_pred(0))
	    return 0;

  new_context = (__dague_zpotrf_L_potrf_zpotrf_task_t *) dague_lifo_pop(
      &context->virtual_processes[0]->execution_units[0]->context_mempool->mempool);
  if (NULL == new_context)
      new_context = (__dague_zpotrf_L_potrf_zpotrf_task_t *) dague_thread_mempool_allocate(
            context->virtual_processes[0]->execution_units[0]->context_mempool);
  new_context->dague_handle = (dague_handle_t *) __dague_handle;
  new_context->function = __dague_handle->super.super.functions_array[zpotrf_L_potrf_zpotrf.function_id];
  new_context->chore_id = 0;
  new_context->locals.k.value = 0;
  DAGUE_LIST_ITEM_SINGLETON(new_context);
  new_context->priority = 0;
  new_context->data.T.data_repo = NULL;
  new_context->data.T.data_in = NULL;
  new_context->data.T.data_out = NULL;
  dague_dependencies_mark_task_as_startup((dague_execution_context_t *) new_context);
  vpid = ((dague_ddesc_t *) __dague_handle->super.dataA)->vpid_of(
                      (dague_ddesc_t *) __dague_handle->super.dataA, 0, 0);
  if (NULL != pready_list[vpid]) {
      dague_list_item_ring_merge((dague_list_item_t *) new_context,
                                 (dague_list_item_t *) (pready_list[vpid]));
  }
  pready_list[vpid] = (dague_execution_context_t *) new_context;
  return 0;
}

static uint32_t nb_local_tasks_loop(struct __dague_zpotrf_L_internal_handle *__dague_handle, tiled_matrix_desc_t *dA, int task_mask)
{
    uint32_t nb = 0;
    int m, n, k;
    if( task_mask & 1 ) {
        /** GEMM */
        for (k = 0; k <= (dA->mt - 3); k += 1) {
            for (m = (k + 2); m <= (dA->mt - 1); m += 1) {
                for (n = (k + 1); n <= (m - 1); n += 1) {
                    if (potrf_zgemm_pred(k, m, n))
                        nb++; 
                }
            }
        }
    }
    if( task_mask & 2 ) {
        /** TRSM */
        for (m = 1; m <= (dA->mt - 1); m += 1) {
            for (k = 0; k <= (m - 1); k += 1) {
                if (potrf_ztrsm_pred(m, k))
                    nb++; 
            }
        }
    }
    if( task_mask & 4 ) {
        /** POTRF */
        for (k = 0; k <= (dA->mt-1); k += 1) {
            if (potrf_zpotrf_pred(k))
                nb++;
        }
    }
    if( task_mask & 8 ) {
        /** HERK */
        for (k = 0; k <= (dA->mt - 2); k += 1) {
            for (m = k+1; m <= (dA->mt - 1); m += 1) {
                if (potrf_zherk_pred(k, m))
                    nb++; 
            }
        }
    }
    return nb;
}
 
static uint32_t my_nb_local_tasks_gemm(struct __dague_zpotrf_L_internal_handle *__dague_handle)
{
    uint32_t nb;
    tiled_matrix_desc_t *dA = &__dague_handle->super.descA;
    if( dA->super.nodes == 1 ) {
        nb = (dA->nt-2) * (dA->nt-1) * (dA->nt) / 6;
    } else {
        nb = nb_local_tasks_loop(__dague_handle, dA, 1);
    }
    return nb;
}
 
static uint32_t my_nb_local_tasks_trsm(struct __dague_zpotrf_L_internal_handle *__dague_handle)
{
    uint32_t nb;
    tiled_matrix_desc_t *dA = &__dague_handle->super.descA;
    if( dA->super.nodes == 1 ) {
        nb = (dA->nt-1) * (dA->nt) / 2;
    } else {
        nb = nb_local_tasks_loop(__dague_handle, dA, 2);
    }
    return nb;
}
 
static uint32_t my_nb_local_tasks_potrf(struct __dague_zpotrf_L_internal_handle *__dague_handle)
{
    uint32_t nb;
    tiled_matrix_desc_t *dA = &__dague_handle->super.descA;
    if( dA->super.nodes == 1 ) {
        nb = dA->nt;
    } else {
        nb = nb_local_tasks_loop(__dague_handle, dA, 4);
    }
    return nb;
}
 
static uint32_t my_nb_local_tasks_herk(struct __dague_zpotrf_L_internal_handle *__dague_handle)
{
    uint32_t nb;
    tiled_matrix_desc_t *dA = &__dague_handle->super.descA;
    if( dA->super.nodes == 1 ) {
        nb = (dA->nt-1)*(dA->nt)/2;
    } else {
        nb = nb_local_tasks_loop(__dague_handle, dA, 8);
    }
    return nb;
}
 
static void *my_potrf_deps_allocate(struct __dague_zpotrf_L_internal_handle *__dague_handle)
{
    tiled_matrix_desc_t *dA = &__dague_handle->super.descA;
    return calloc(dA->nt, sizeof(dague_dependency_t));
}
 
static void  my_potrf_deps_free(struct __dague_zpotrf_L_internal_handle *__dague_handle, void *deps)
{
    (void)__dague_handle;
    free(deps);
}
 
static dague_dependency_t *my_potrf_deps_find(const dague_handle_t *dague_handle,
                                              const dague_execution_context_t* restrict exec_context)
{
    const __dague_dpotrf_L_potrf_dpotrf_task_t * restrict potrf_ctx = (const __dague_dpotrf_L_potrf_dpotrf_task_t *restrict)exec_context;
    dague_dependency_t *deparray;
    deparray = (dague_dependency_t *)dague_handle->dependencies_array[ exec_context->function->function_id ];
    return &deparray[potrf_ctx->locals.k.value];
}

 %}
